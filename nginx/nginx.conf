user nginx;
worker_processes auto;
worker_rlimit_nofile 100000;

events
{
  worker_connections 1024;
  multi_accept on;
  use epoll;
}
http
{
  include /etc/nginx/mime.types;
  default_type application/octet-stream;

  limit_req_zone "$http_cf_connecting_ip" zone=zone:100m rate=50r/s;
  log_format main '$http_cf_connecting_ip $remote_user [$time_local] "$request" '
  '$status $body_bytes_sent '
  '"$http_user_agent" "$http_authorization"';

  underscores_in_headers on;

  access_log /var/log/nginx/access.log main;
  error_log /var/log/nginx/error.log;
  client_max_body_size 40M;

  # send headers in one peace, its better then sending them one by one
  tcp_nopush on;

  # don't buffer data sent, good for small data bursts in real time
  tcp_nodelay on;

  # allow the server to close connection on non responding client, this will free up memory
  reset_timedout_connection on;

  # Security reasons, turn off nginx versions
  server_tokens off;

  server
  {
    server_name chat.yusufcirak.net;

    # listen 80;
    # listen [::]:80;

    listen 443 ssl http2;
    listen [::]:443 ssl http2;

    # listen 443 ssl http3 reuseport;
    # listen [::]:443 ssl http3 reuseport;

    limit_req zone=zone burst=50 nodelay;



    # Request buffering in not currently supported for HTTP/3.
    #proxy_request_buffering off;
    
    # Add Alt-Svc header to negotiate HTTP/3.
    # add_header alt-svc 'h3-27=":443"; ma=86400';

    ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt;
    ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key;

    # ssl_quic on;
    ssl_early_data on;

    ssl_protocols TLSv1.2 TLSv1.3;

    ssl_ciphers "TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384";
    ssl_prefer_server_ciphers on;

    location /_api
    {
        proxy_pass http://backend:80;
    }

    location /_chat
    {
        proxy_buffering off;
        proxy_pass http://backend:80;
        
    }

    location /
    {
        # serve static files from the nginx container
        sendfile_max_chunk 1m;
        proxy_pass http://frontend:80;
    }

  }
}
